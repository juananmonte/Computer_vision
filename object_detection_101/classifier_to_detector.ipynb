{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier to detector using image pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back before deep learning-based object detectors, the state-of-the-art was to use HOG + Linear SVM to detect objects in an image.\n",
    "\n",
    "We’ll be borrowing elements from HOG + Linear SVM to convert any deep neural network image classifier into an object detector.\n",
    "\n",
    "**1st key ingredient:** \n",
    "\n",
    "from HOG + Linear SVM is to use image pyramids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://929687.smushcdn.com/2633864/wp-content/uploads/2020/06/keras_classifier_object_detector_pyramid_example_2.png?lossy%3D1%26strip%3D1%26webp%3D1)\n",
    "\n",
    "Utilizing an image pyramid allows us to find objects in images at different scales (i.e., sizes) of an image (Figure 2).\n",
    "\n",
    "At the bottom of the pyramid, we have the original image at its original size (in terms of width and height).\n",
    "\n",
    "And at each subsequent layer, the image is resized (subsampled) and optionally smoothed (usually via Gaussian blurring).\n",
    "\n",
    "The image is progressively subsampled until some stopping criterion is met, which is normally when a minimum size has been reached and no further subsampling needs to take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd key ingredient**\n",
    "\n",
    "Sliding Windows\n",
    "\n",
    "![](https://929687.smushcdn.com/2633864/wp-content/uploads/2014/10/sliding_window_example.gif?size%3D256x377%26lossy%3D1%26strip%3D1%26webp%3D1)\n",
    "\n",
    "a sliding window is a fixed-size rectangle that slides from left-to-right and top-to-bottom within an image.\n",
    "\n",
    "At each stop of the window we would:\n",
    "\n",
    "1. Extract the ROI\n",
    "2. Pass it through our image classifier (ex., Linear SVM, CNN, etc.)\n",
    "3. Obtain the output predictions\n",
    "\n",
    ">> **Combined with image pyramids, sliding windows allow us to localize objects at different locations and multiple scales of the input image:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3rd key ingredient**\n",
    "\n",
    "Non maxima Supression\n",
    "\n",
    "When performing object detection, our object detector will typically produce multiple, overlapping bounding boxes surrounding an object in an image.\n",
    "\n",
    "This behavior is totally normal — it simply implies that as the sliding window approaches an image, our classifier component is returning larger and larger probabilities of a positive detection.\n",
    "\n",
    "Of course, multiple bounding boxes pose a problem — there’s only one object there, and we somehow need to collapse/remove the extraneous bounding boxes.\n",
    "\n",
    "**The solution to the problem is to apply non-maxima suppression (NMS), which collapses weak, overlapping bounding boxes in favor of the more confident ones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](https://929687.smushcdn.com/2633864/wp-content/uploads/2020/06/keras_classifier_object_detector_steps.png?lossy%3D1%26strip%3D1%26webp%3D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Tensorflow\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "#--Others\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import argparse \n",
    "import imutils\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the sliding window and the image pyramid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    #ws: The window size defines the width and height (in pixels) of the window we are going to extract \n",
    "    #from our image\n",
    "    #slide a window through the image. Complete the rows (x) before moving down the columns (y) of the image\n",
    "    for y in range(0, image.shape[0]-ws[1], step): #substract ws so we get the difference that is the space we will pass through\n",
    "        for x in range(0, image.shape[1]-ws[0], step):\n",
    "            # step size, which indicates how many pixels we are going to “skip” in both the (x, y) directions.\n",
    "            #ususally goes from 4 to 8 pixels\n",
    "            #yield the current window (generator)\n",
    "            yield (x, y, image[y:y+ws[1], x:x+ws[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_pyramid(image, scale=1.5, minSize=(224,224)):\n",
    "    #Yield the original image\n",
    "    yield image\n",
    "\n",
    "    #Keep lopping over the image pyramid\n",
    "    while True:\n",
    "        #Get the dimentions of the next image of the pyramid\n",
    "        w = int(image.shape[1]/scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        \n",
    "        #if the resized images reaches the minimum supplied size then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]: #remember that cv2 reads height first. so image.shape[0] is height\n",
    "            break\n",
    "\n",
    "        #yield the next image in the pyramid\n",
    "        yield image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model with it's weights\n",
    "model = ResNet50(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = cv2.imread('/media/juan/juan1/pyimage_univ/object_detect_201/classifier-to-detector/images/hummingbird.jpg')\n",
    "orig = imutils.resize(orig, width=600)\n",
    "(H,W) = orig.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the pyramid\n",
    "pyramid = image_pyramid(orig, scale=1.5, minSize=(200,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two lists:\n",
    "#Rois: will contain all the ROI's generated from the pyramid and sliding window\n",
    "#Locs: contains the x, y coordinates of where those ROI's were on the original image\n",
    "rois, locs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_size = (200,150)\n",
    "input_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through all images in the pyramid\n",
    "for image in pyramid:\n",
    "\n",
    "    #Determine the scale factor between the original image dimensions and the current layer of the pyramid\n",
    "    scale = W/float(image.shape[1])\n",
    "\n",
    "    #For each layer of the image pyramid, loop ober the sliding window locations\n",
    "    for (x, y, roiOrig) in sliding_window(image, 16, roi_size):\n",
    "        #Scale the (x, y) of the ROI with respect to the original image dimensions\n",
    "        x = int(x*scale)\n",
    "        y = int(y*scale)\n",
    "        w = int(roi_size[0]*scale)\n",
    "        h = int(roi_size[1]*scale)\n",
    "\n",
    "        #Take the roi and pre-proces it soo we can classify the region with keras/tf\n",
    "        roi = cv2.resize(roiOrig, input_size) #resize to the dimensions used by resnet50\n",
    "        roi = img_to_array(roi)\n",
    "        roi = preprocess_input(roi)\n",
    "\n",
    "        #Update the list of ROIs and coordinates\n",
    "        rois.append(roi)\n",
    "        locs.append((x, y, x+w, y+h)) #remember to add the difference\n",
    "\n",
    "        #Visualize the sliding window and roi in real time\n",
    "        clone = orig.copy()\n",
    "        cv2.rectangle(clone, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "        cv2.imshow(\"visualization\", clone)\n",
    "        cv2.imshow('roi', roiOrig)\n",
    "        cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the rois list into a float32 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = np.array(rois, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 224, 224, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the ROIs and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 17s 1s/step\n",
      "[INFO] classifying ROIs took 17.53 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "preds = model.predict(rois)\n",
    "end = time.time()\n",
    "print(\"[INFO] classifying ROIs took {:.2f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the predictions and initialize a dictionary which maps class\n",
    "# labels (keys) to any ROIs associated with that label (values)\n",
    "preds = imagenet_utils.decode_predictions(preds, top=1) #all the labels used to train the resnet50 model\n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over the predictions\n",
    "for (i,p) in enumerate(preds):\n",
    "    #grab the prediction information for the current ROI\n",
    "    (imagenetID, label, prob) = p[0]\n",
    "\n",
    "    #filter out weak detections. Use a treshold: the minimum accepted probabilty\n",
    "    if prob>= 0.9:\n",
    "        #grab the bounding box associated to the probability \n",
    "        box = locs[1]\n",
    "\n",
    "        #Get the label associated to the probability\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hummingbird': [((16, 0, 216, 150), 0.98895955),\n",
       "  ((16, 0, 216, 150), 0.9960862),\n",
       "  ((16, 0, 216, 150), 0.9686435),\n",
       "  ((16, 0, 216, 150), 0.9984379),\n",
       "  ((16, 0, 216, 150), 0.9995752),\n",
       "  ((16, 0, 216, 150), 0.9992683),\n",
       "  ((16, 0, 216, 150), 0.9980788),\n",
       "  ((16, 0, 216, 150), 0.9991916),\n",
       "  ((16, 0, 216, 150), 0.9996457),\n",
       "  ((16, 0, 216, 150), 0.9991525),\n",
       "  ((16, 0, 216, 150), 0.99810106),\n",
       "  ((16, 0, 216, 150), 0.9958823),\n",
       "  ((16, 0, 216, 150), 0.9972958),\n",
       "  ((16, 0, 216, 150), 0.9951442),\n",
       "  ((16, 0, 216, 150), 0.9589257),\n",
       "  ((16, 0, 216, 150), 0.9922621),\n",
       "  ((16, 0, 216, 150), 0.9608538),\n",
       "  ((16, 0, 216, 150), 0.9903033),\n",
       "  ((16, 0, 216, 150), 0.9993269),\n",
       "  ((16, 0, 216, 150), 0.9986399),\n",
       "  ((16, 0, 216, 150), 0.9982065),\n",
       "  ((16, 0, 216, 150), 0.9993145),\n",
       "  ((16, 0, 216, 150), 0.9992097),\n",
       "  ((16, 0, 216, 150), 0.99889195),\n",
       "  ((16, 0, 216, 150), 0.9905469),\n",
       "  ((16, 0, 216, 150), 0.98441863),\n",
       "  ((16, 0, 216, 150), 0.96884245),\n",
       "  ((16, 0, 216, 150), 0.95047325),\n",
       "  ((16, 0, 216, 150), 0.95878476),\n",
       "  ((16, 0, 216, 150), 0.98512286),\n",
       "  ((16, 0, 216, 150), 0.998203),\n",
       "  ((16, 0, 216, 150), 0.9979633),\n",
       "  ((16, 0, 216, 150), 0.99927926),\n",
       "  ((16, 0, 216, 150), 0.999389),\n",
       "  ((16, 0, 216, 150), 0.99968374),\n",
       "  ((16, 0, 216, 150), 0.9995932),\n",
       "  ((16, 0, 216, 150), 0.99679905),\n",
       "  ((16, 0, 216, 150), 0.99381495),\n",
       "  ((16, 0, 216, 150), 0.9965371),\n",
       "  ((16, 0, 216, 150), 0.94719887),\n",
       "  ((16, 0, 216, 150), 0.9806694),\n",
       "  ((16, 0, 216, 150), 0.99582326),\n",
       "  ((16, 0, 216, 150), 0.97333455),\n",
       "  ((16, 0, 216, 150), 0.99871856),\n",
       "  ((16, 0, 216, 150), 0.9991488),\n",
       "  ((16, 0, 216, 150), 0.9991551),\n",
       "  ((16, 0, 216, 150), 0.9979331),\n",
       "  ((16, 0, 216, 150), 0.9981365),\n",
       "  ((16, 0, 216, 150), 0.99916345),\n",
       "  ((16, 0, 216, 150), 0.99893564),\n",
       "  ((16, 0, 216, 150), 0.9856134),\n",
       "  ((16, 0, 216, 150), 0.99753225),\n",
       "  ((16, 0, 216, 150), 0.96229124),\n",
       "  ((16, 0, 216, 150), 0.9483297),\n",
       "  ((16, 0, 216, 150), 0.9810344),\n",
       "  ((16, 0, 216, 150), 0.9973362),\n",
       "  ((16, 0, 216, 150), 0.9840146),\n",
       "  ((16, 0, 216, 150), 0.9976767),\n",
       "  ((16, 0, 216, 150), 0.9997935),\n",
       "  ((16, 0, 216, 150), 0.9995865),\n",
       "  ((16, 0, 216, 150), 0.99927354),\n",
       "  ((16, 0, 216, 150), 0.9994496),\n",
       "  ((16, 0, 216, 150), 0.9989598),\n",
       "  ((16, 0, 216, 150), 0.9993853),\n",
       "  ((16, 0, 216, 150), 0.99271643),\n",
       "  ((16, 0, 216, 150), 0.99653935),\n",
       "  ((16, 0, 216, 150), 0.98713505),\n",
       "  ((16, 0, 216, 150), 0.9806549),\n",
       "  ((16, 0, 216, 150), 0.99772197),\n",
       "  ((16, 0, 216, 150), 0.9985307),\n",
       "  ((16, 0, 216, 150), 0.99760383),\n",
       "  ((16, 0, 216, 150), 0.9990508),\n",
       "  ((16, 0, 216, 150), 0.99970603),\n",
       "  ((16, 0, 216, 150), 0.99989754),\n",
       "  ((16, 0, 216, 150), 0.9998548),\n",
       "  ((16, 0, 216, 150), 0.99963087),\n",
       "  ((16, 0, 216, 150), 0.99892694),\n",
       "  ((16, 0, 216, 150), 0.99952394),\n",
       "  ((16, 0, 216, 150), 0.9942161),\n",
       "  ((16, 0, 216, 150), 0.99030185),\n",
       "  ((16, 0, 216, 150), 0.9787992),\n",
       "  ((16, 0, 216, 150), 0.9941202),\n",
       "  ((16, 0, 216, 150), 0.99965656),\n",
       "  ((16, 0, 216, 150), 0.9935221),\n",
       "  ((16, 0, 216, 150), 0.9991001),\n",
       "  ((16, 0, 216, 150), 0.99939716),\n",
       "  ((16, 0, 216, 150), 0.9996212),\n",
       "  ((16, 0, 216, 150), 0.99920744),\n",
       "  ((16, 0, 216, 150), 0.99941695),\n",
       "  ((16, 0, 216, 150), 0.99857163),\n",
       "  ((16, 0, 216, 150), 0.99269545),\n",
       "  ((16, 0, 216, 150), 0.9857854),\n",
       "  ((16, 0, 216, 150), 0.94895524),\n",
       "  ((16, 0, 216, 150), 0.98557705),\n",
       "  ((16, 0, 216, 150), 0.9985758),\n",
       "  ((16, 0, 216, 150), 0.99968076),\n",
       "  ((16, 0, 216, 150), 0.99948436),\n",
       "  ((16, 0, 216, 150), 0.9997623),\n",
       "  ((16, 0, 216, 150), 0.9991269),\n",
       "  ((16, 0, 216, 150), 0.9997635),\n",
       "  ((16, 0, 216, 150), 0.9993936),\n",
       "  ((16, 0, 216, 150), 0.99932116),\n",
       "  ((16, 0, 216, 150), 0.9960817),\n",
       "  ((16, 0, 216, 150), 0.95795447),\n",
       "  ((16, 0, 216, 150), 0.9837632),\n",
       "  ((16, 0, 216, 150), 0.992666),\n",
       "  ((16, 0, 216, 150), 0.99287516),\n",
       "  ((16, 0, 216, 150), 0.98583513),\n",
       "  ((16, 0, 216, 150), 0.99692065),\n",
       "  ((16, 0, 216, 150), 0.95178175),\n",
       "  ((16, 0, 216, 150), 0.9425984),\n",
       "  ((16, 0, 216, 150), 0.9990422),\n",
       "  ((16, 0, 216, 150), 0.99897075),\n",
       "  ((16, 0, 216, 150), 0.99590683),\n",
       "  ((16, 0, 216, 150), 0.99845976),\n",
       "  ((16, 0, 216, 150), 0.9918723),\n",
       "  ((16, 0, 216, 150), 0.99330956),\n",
       "  ((16, 0, 216, 150), 0.93983215),\n",
       "  ((16, 0, 216, 150), 0.9535317),\n",
       "  ((16, 0, 216, 150), 0.9958008),\n",
       "  ((16, 0, 216, 150), 0.9544991),\n",
       "  ((16, 0, 216, 150), 0.9627649),\n",
       "  ((16, 0, 216, 150), 0.90510905),\n",
       "  ((16, 0, 216, 150), 0.98421645),\n",
       "  ((16, 0, 216, 150), 0.93709713),\n",
       "  ((16, 0, 216, 150), 0.9725704),\n",
       "  ((16, 0, 216, 150), 0.99751025),\n",
       "  ((16, 0, 216, 150), 0.9997802),\n",
       "  ((16, 0, 216, 150), 0.99981713),\n",
       "  ((16, 0, 216, 150), 0.9994451),\n",
       "  ((16, 0, 216, 150), 0.99882215),\n",
       "  ((16, 0, 216, 150), 0.9990193),\n",
       "  ((16, 0, 216, 150), 0.9991637),\n",
       "  ((16, 0, 216, 150), 0.99836123),\n",
       "  ((16, 0, 216, 150), 0.9967971),\n",
       "  ((16, 0, 216, 150), 0.99947375),\n",
       "  ((16, 0, 216, 150), 0.9999692),\n",
       "  ((16, 0, 216, 150), 0.99983215),\n",
       "  ((16, 0, 216, 150), 0.9999532),\n",
       "  ((16, 0, 216, 150), 0.999841),\n",
       "  ((16, 0, 216, 150), 0.99957365),\n",
       "  ((16, 0, 216, 150), 0.99860865),\n",
       "  ((16, 0, 216, 150), 0.9993585),\n",
       "  ((16, 0, 216, 150), 0.99599123),\n",
       "  ((16, 0, 216, 150), 0.99972785),\n",
       "  ((16, 0, 216, 150), 0.9997666),\n",
       "  ((16, 0, 216, 150), 0.99990255),\n",
       "  ((16, 0, 216, 150), 0.99992687),\n",
       "  ((16, 0, 216, 150), 0.9997575),\n",
       "  ((16, 0, 216, 150), 0.99966884),\n",
       "  ((16, 0, 216, 150), 0.99964607),\n",
       "  ((16, 0, 216, 150), 0.99990064),\n",
       "  ((16, 0, 216, 150), 0.9947562),\n",
       "  ((16, 0, 216, 150), 0.9997754),\n",
       "  ((16, 0, 216, 150), 0.9999495),\n",
       "  ((16, 0, 216, 150), 0.99995285),\n",
       "  ((16, 0, 216, 150), 0.99997574),\n",
       "  ((16, 0, 216, 150), 0.9997122),\n",
       "  ((16, 0, 216, 150), 0.9996032),\n",
       "  ((16, 0, 216, 150), 0.99835527),\n",
       "  ((16, 0, 216, 150), 0.999605),\n",
       "  ((16, 0, 216, 150), 0.985304),\n",
       "  ((16, 0, 216, 150), 0.9977995),\n",
       "  ((16, 0, 216, 150), 0.9998641),\n",
       "  ((16, 0, 216, 150), 0.999784),\n",
       "  ((16, 0, 216, 150), 0.99980974),\n",
       "  ((16, 0, 216, 150), 0.9989543),\n",
       "  ((16, 0, 216, 150), 0.9982037),\n",
       "  ((16, 0, 216, 150), 0.99710166),\n",
       "  ((16, 0, 216, 150), 0.9973171),\n",
       "  ((16, 0, 216, 150), 0.93448955)],\n",
       " 'jellyfish': [((16, 0, 216, 150), 0.9781972),\n",
       "  ((16, 0, 216, 150), 0.96237475),\n",
       "  ((16, 0, 216, 150), 0.94422024),\n",
       "  ((16, 0, 216, 150), 0.95558214),\n",
       "  ((16, 0, 216, 150), 0.96300286),\n",
       "  ((16, 0, 216, 150), 0.9097649),\n",
       "  ((16, 0, 216, 150), 0.93312275),\n",
       "  ((16, 0, 216, 150), 0.97451043),\n",
       "  ((16, 0, 216, 150), 0.9768156),\n",
       "  ((16, 0, 216, 150), 0.9076598),\n",
       "  ((16, 0, 216, 150), 0.915639),\n",
       "  ((16, 0, 216, 150), 0.9013162),\n",
       "  ((16, 0, 216, 150), 0.94694734),\n",
       "  ((16, 0, 216, 150), 0.9573781),\n",
       "  ((16, 0, 216, 150), 0.9020326),\n",
       "  ((16, 0, 216, 150), 0.9386414)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop obrt sl bounding boxes and the current label\n",
    "for(box, prob) in labels[label]:\n",
    "    #draw the bounding box on the image\n",
    "    (startx, starty, endX, endY) = box\n",
    "    cv2.rectangle(clone, (startx, starty), (endX, endY), (0,255,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results *before* applying non-maxima suppression, then\n",
    "\t# clone the image again so we can display the results *after*\n",
    "\t# applying non-maxima suppression\n",
    "cv2.imshow(\"Before\", clone)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "clone = orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply non maxima supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the bounding boxes and associated prediction probabilities, then apply non-maxima suppression\n",
    "boxes = np.array([p[0] for p in labels[label]])\n",
    "proba = np.array([p[1] for p in labels[label]])\n",
    "boxes = non_max_suppression(boxes, proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all bounding boxes that were kept after applying non-maxima suppression\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "\tcv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\tcv2.putText(clone, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "\t# show the output after apply non-maxima suppression\n",
    "\tcv2.imshow(\"After\", clone)\n",
    "\tcv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da695c087d6368c9b262a08c2ebf368542a47f83b2881693eefdf5ebab9d0ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
