{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, Reshape, Flatten, Conv2DTranspose, LeakyReLU, Dropout, Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points from the latent space. We can achieve this by calling the randn() NumPy function for generating arrays of random numbers drawn from a standard Gaussian. \n",
    "\n",
    "**The array of random numbers can then be reshaped into samples, that is n rows with 100 elements per row.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate ✬real✬ class labels (1)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create ✬fake✬ class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare mnist training images\n",
    "def load_real_samples():\n",
    "    # load mnist dataset\n",
    "    (trainX, _), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = np.expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define discriminator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: Image with one channel and 28 × 28 pixels in size.\n",
    "- Outputs: Binary classification, likelihood the sample is real (or fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained to minimize the binary cross-entropy loss function, appropriate for binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    input = Input(shape=(28,28,1))\n",
    "    x = Conv2D(64, (3,3), strides=(2,2), padding=\"same\")(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Conv2D(64, (3,3), strides=(2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model = Model(inputs= input, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discriminator = define_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        640       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 3137      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,705\n",
      "Trainable params: 40,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ge the data\n",
    "(trainX, _), (_,_) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images are 2D arrays of pixels and convolutional neural networks expect 3D arrays of\n",
    "# images as input, where each image has one or more channels. \n",
    "# specify the final dimension for the channels-last image format.\n",
    "X = np.expand_dims(trainX, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the images. \n",
    "X = X.astype('float32')\n",
    "X = trainX/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fake but plausible images of handwritten digits. It does this by taking a point from the latent space as input and outputting a square\n",
    "\n",
    "The latent space is an arbitrarily defined vector space of Gaussian-distributed values, e.g. 100 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs: Point in latent space, e.g. a 100 element vector of Gaussian random numbers.\n",
    "- Outputs: Two-dimensional square grayscale image of 28 × 28 pixels with pixel values in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    input = Input(shape=(latent_dim))\n",
    "    n_nodes = 128*7*7\n",
    "    x = Dense(n_nodes)(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Reshape((7, 7, 128))(x)\n",
    "    #Upsample to 14x14\n",
    "    x = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    #Upsample to  28x28\n",
    "    x = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    #final\n",
    "    output = Conv2D(1, (7,7), activation='sigmoid', padding='same')(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = define_generator(latent_dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6272)              633472    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 6272)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 128)      262272    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 128)      262272    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 1)         6273      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,164,289\n",
      "Trainable params: 1,164,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights in the generator model are updated based on the performance of the discriminator\n",
    "model. When the discriminator is good at detecting fake samples, the generator is updated more,\n",
    "and when the discriminator model is relatively poor or confused when detecting fake samples,\n",
    "the generator model is updated less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the models\n",
    "\n",
    "d_model = define_discriminator()\n",
    "g_model = define_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define gan training\n",
    "\n",
    "def define_gan(g_model, d_model):\n",
    "    #make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    #Connect them\n",
    "    model = Sequential()\n",
    "    #add generator\n",
    "    model.add(g_model)\n",
    "    #add discriminator\n",
    "    model.add(d_model)\n",
    "    #compile the model\n",
    "    opt = Adam(learning_rate=0.0002, decay=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer = opt)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model\n",
    "gan_model = define_gan(g_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_3 (Functional)        (None, 28, 28, 1)         1164289   \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 1)                 40705     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,204,994\n",
      "Trainable params: 1,164,289\n",
      "Non-trainable params: 40,705\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the composite model involves **generating a batch worth of points in the latent space via the generate latent points() function in the previous section, and class = 1 labels\n",
    "and calling the train on batch() function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan_model, latent_dim, n_epochs=100, n_batch=256):\n",
    "    #manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        #prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        #create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        #update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n=10):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "    # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "        #save plot to file\n",
    "        filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "    bat_per_epoch = int(dataset.shape[0]/n_batch)\n",
    "    half_batch = int(n_batch/2)\n",
    "    #manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        #enumerate batches  over the training set\n",
    "        for j in range(bat_per_epoch):\n",
    "            #get random 'real' examples:\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            #get random fake examples:\n",
    "            X_fake, y_fake, = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            #create training set for the discriminator\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            #update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X,y)\n",
    "            #prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            #create inverted labels for the fake samples\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            #update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            #summarize loss on this batch\n",
    "            print('>%d, %d/%d. d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epoch, d_loss, g_loss ))\n",
    "        if (i+1)%10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = define_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = define_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model = define_gan(g_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 180ms/step\n",
      ">1, 1/234. d=0.697, g=0.726\n",
      "4/4 [==============================] - 1s 167ms/step\n",
      ">1, 2/234. d=0.692, g=0.738\n",
      "4/4 [==============================] - 1s 254ms/step\n",
      ">1, 3/234. d=0.686, g=0.752\n",
      "4/4 [==============================] - 1s 169ms/step\n",
      ">1, 4/234. d=0.680, g=0.763\n",
      "4/4 [==============================] - 1s 266ms/step\n",
      ">1, 5/234. d=0.674, g=0.779\n",
      "4/4 [==============================] - 1s 288ms/step\n",
      ">1, 6/234. d=0.667, g=0.796\n",
      "4/4 [==============================] - 1s 257ms/step\n",
      ">1, 7/234. d=0.660, g=0.809\n",
      "4/4 [==============================] - 1s 179ms/step\n",
      ">1, 8/234. d=0.650, g=0.827\n",
      "4/4 [==============================] - 1s 174ms/step\n",
      ">1, 9/234. d=0.648, g=0.848\n",
      "4/4 [==============================] - 1s 166ms/step\n",
      ">1, 10/234. d=0.639, g=0.869\n",
      "4/4 [==============================] - 1s 168ms/step\n",
      ">1, 11/234. d=0.630, g=0.891\n",
      "4/4 [==============================] - 1s 158ms/step\n",
      ">1, 12/234. d=0.622, g=0.913\n",
      "4/4 [==============================] - 1s 158ms/step\n",
      ">1, 13/234. d=0.615, g=0.940\n",
      "4/4 [==============================] - 1s 168ms/step\n",
      ">1, 14/234. d=0.604, g=0.964\n",
      "4/4 [==============================] - 1s 150ms/step\n",
      ">1, 15/234. d=0.596, g=1.003\n",
      "4/4 [==============================] - 1s 151ms/step\n",
      ">1, 16/234. d=0.590, g=1.028\n",
      "4/4 [==============================] - 1s 164ms/step\n",
      ">1, 17/234. d=0.573, g=1.065\n",
      "4/4 [==============================] - 1s 161ms/step\n",
      ">1, 18/234. d=0.563, g=1.102\n",
      "4/4 [==============================] - 1s 178ms/step\n",
      ">1, 19/234. d=0.553, g=1.139\n",
      "4/4 [==============================] - 1s 150ms/step\n",
      ">1, 20/234. d=0.543, g=1.173\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 21/234. d=0.527, g=1.216\n",
      "4/4 [==============================] - 1s 157ms/step\n",
      ">1, 22/234. d=0.519, g=1.265\n",
      "4/4 [==============================] - 1s 160ms/step\n",
      ">1, 23/234. d=0.502, g=1.306\n",
      "4/4 [==============================] - 1s 159ms/step\n",
      ">1, 24/234. d=0.494, g=1.345\n",
      "4/4 [==============================] - 1s 155ms/step\n",
      ">1, 25/234. d=0.477, g=1.384\n",
      "4/4 [==============================] - 1s 159ms/step\n",
      ">1, 26/234. d=0.462, g=1.449\n",
      "4/4 [==============================] - 1s 150ms/step\n",
      ">1, 27/234. d=0.446, g=1.479\n",
      "4/4 [==============================] - 1s 155ms/step\n",
      ">1, 28/234. d=0.434, g=1.537\n",
      "4/4 [==============================] - 1s 150ms/step\n",
      ">1, 29/234. d=0.421, g=1.595\n",
      "4/4 [==============================] - 1s 165ms/step\n",
      ">1, 30/234. d=0.404, g=1.627\n",
      "4/4 [==============================] - 1s 162ms/step\n",
      ">1, 31/234. d=0.389, g=1.682\n",
      "4/4 [==============================] - 1s 154ms/step\n",
      ">1, 32/234. d=0.378, g=1.727\n",
      "4/4 [==============================] - 1s 162ms/step\n",
      ">1, 33/234. d=0.360, g=1.795\n",
      "4/4 [==============================] - 1s 172ms/step\n",
      ">1, 34/234. d=0.346, g=1.831\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 35/234. d=0.335, g=1.889\n",
      "4/4 [==============================] - 1s 154ms/step\n",
      ">1, 36/234. d=0.321, g=1.908\n",
      "4/4 [==============================] - 1s 158ms/step\n",
      ">1, 37/234. d=0.298, g=1.962\n",
      "4/4 [==============================] - 1s 161ms/step\n",
      ">1, 38/234. d=0.286, g=2.018\n",
      "4/4 [==============================] - 1s 161ms/step\n",
      ">1, 39/234. d=0.272, g=2.058\n",
      "4/4 [==============================] - 1s 154ms/step\n",
      ">1, 40/234. d=0.261, g=2.128\n",
      "4/4 [==============================] - 1s 158ms/step\n",
      ">1, 41/234. d=0.245, g=2.182\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 42/234. d=0.225, g=2.220\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 43/234. d=0.220, g=2.273\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 44/234. d=0.209, g=2.327\n",
      "4/4 [==============================] - 1s 160ms/step\n",
      ">1, 45/234. d=0.194, g=2.419\n",
      "4/4 [==============================] - 1s 151ms/step\n",
      ">1, 46/234. d=0.175, g=2.469\n",
      "4/4 [==============================] - 1s 149ms/step\n",
      ">1, 47/234. d=0.161, g=2.531\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 48/234. d=0.158, g=2.593\n",
      "4/4 [==============================] - 1s 158ms/step\n",
      ">1, 49/234. d=0.145, g=2.630\n",
      "4/4 [==============================] - 1s 159ms/step\n",
      ">1, 50/234. d=0.138, g=2.679\n",
      "4/4 [==============================] - 1s 153ms/step\n",
      ">1, 51/234. d=0.130, g=2.765\n",
      "4/4 [==============================] - 1s 151ms/step\n",
      ">1, 52/234. d=0.122, g=2.812\n",
      "4/4 [==============================] - 1s 153ms/step\n",
      ">1, 53/234. d=0.113, g=2.846\n",
      "4/4 [==============================] - 1s 202ms/step\n",
      ">1, 54/234. d=0.104, g=2.918\n",
      "4/4 [==============================] - 1s 161ms/step\n",
      ">1, 55/234. d=0.101, g=2.980\n",
      "4/4 [==============================] - 1s 161ms/step\n",
      ">1, 56/234. d=0.100, g=3.056\n",
      "4/4 [==============================] - 1s 162ms/step\n",
      ">1, 57/234. d=0.089, g=3.064\n",
      "4/4 [==============================] - 1s 167ms/step\n",
      ">1, 58/234. d=0.081, g=3.131\n",
      "4/4 [==============================] - 1s 157ms/step\n",
      ">1, 59/234. d=0.080, g=3.187\n",
      "4/4 [==============================] - 1s 157ms/step\n",
      ">1, 60/234. d=0.073, g=3.213\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 61/234. d=0.071, g=3.283\n",
      "4/4 [==============================] - 1s 190ms/step\n",
      ">1, 62/234. d=0.069, g=3.280\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 63/234. d=0.065, g=3.383\n",
      "4/4 [==============================] - 1s 154ms/step\n",
      ">1, 64/234. d=0.062, g=3.392\n",
      "4/4 [==============================] - 1s 151ms/step\n",
      ">1, 65/234. d=0.054, g=3.468\n",
      "4/4 [==============================] - 1s 155ms/step\n",
      ">1, 66/234. d=0.053, g=3.522\n",
      "4/4 [==============================] - 1s 160ms/step\n",
      ">1, 67/234. d=0.049, g=3.554\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 68/234. d=0.048, g=3.648\n",
      "4/4 [==============================] - 1s 177ms/step\n",
      ">1, 69/234. d=0.047, g=3.646\n",
      "4/4 [==============================] - 1s 163ms/step\n",
      ">1, 70/234. d=0.048, g=3.721\n",
      "4/4 [==============================] - 1s 167ms/step\n"
     ]
    }
   ],
   "source": [
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da695c087d6368c9b262a08c2ebf368542a47f83b2881693eefdf5ebab9d0ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
