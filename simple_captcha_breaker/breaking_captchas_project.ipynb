{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from break_capt import minivggnet\n",
    "#---sklearn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "#---tensorflow\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from imutils import contours\n",
    "from imutils import paths\n",
    "#---others\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imutils\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the dataset can be downaloaded from \"https://www.e-zpassny.com/vector/jcaptcha.do\"\n",
    "imagePaths = list(paths.list_images(\"...\\downloads\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract the numbers from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {}\n",
    "#preprocess all the images\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    try:\n",
    "        image = cv2.imread(imagePaths)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE) #pad the images so, when extracting the numbers, the border of the image is also kept\n",
    "        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV |cv2.THRESH_OTSU)[1] #combine thresholds to obtaing the shape and then soften the noise\n",
    "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        cnts = sorted(cnts, key=cv2.contourArea,reverse=True)[:4]\n",
    "    for c in cnts:\n",
    "        (x,y,w,h) = cv2.boundingRect(c) #for each number, draw a bounding box and extract its coordinates\n",
    "        roi= gray[y-5 : y+h+5 , x-5 : x+w+5 ] #define the roi by adding some extra space to the image\n",
    "        #Assign labels to the numbers\n",
    "        cv2.imshow(\"ROI\", imutils.resize(roi, width=28))\n",
    "        key = cv2.waitKey(0) #assign a number tothe hey variable according to the number pressed with the keyboard\n",
    "        if key == ord(\"`\"):# if key ` is presed, ignore\n",
    "            print(\"[INFO] ignoring character\")\n",
    "            continue\n",
    "        key = chr(key).upper()\n",
    "        dirPath = os.path.sep.join([\"...\\annotations\", key])\n",
    "        #if the path does not exist, create it\n",
    "        if not os.path.exists(dirPath):\n",
    "            os.makedirs(dirPath)\n",
    "        count = counts.get(key, 1)\n",
    "        print(count)\n",
    "        p = os.path.sep.join([dirPath, \"{}.png\".format(str(count).zfill(6))]) #add six 0 to the name of the image by default and fill them with the numbers\n",
    "        cv2.imwrite(p, roi) #save the image\n",
    "        counts[key] = count + 1 #up the count so to not replace the already saved images\n",
    "    cv2.destroyAllWindows() #so it doesn't get stuck\n",
    "\n",
    "    # except KeyboardInterrupt:\n",
    "    #     print(\"[INFO] manually leaving script\")\n",
    "    #     break\n",
    "\n",
    "\t# # an unknown error has occurred for this particular image\n",
    "\t# except:\n",
    "\t# \tprint(\"[INFO] skipping image...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, width, height):\n",
    "\t(h, w) = image.shape[:2]\n",
    "\tif w > h: #in case the width is larger than the heigh. So to avoid deformation in size\n",
    "\t\timage = imutils.resize(image, width=width)\n",
    "\telse:\n",
    "\t\timage = imutils.resize(image, height=height)\n",
    "\n",
    "\tpadW = int((width - image.shape[1]) / 2.0) #add some extrapadding on both sides\n",
    "\tpadH = int((height - image.shape[0]) / 2.0)\n",
    "\n",
    "\timage = cv2.copyMakeBorder(image, padH, padH, padW, padW, cv2.BORDER_REPLICATE)\n",
    "\timage = cv2.resize(image, (width, height)) #resize one more time just to avoid any issues\n",
    "    \n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extarct the labels\n",
    "for imagePath in paths.list_images(\"...\\annotations\"):\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = preprocess(image, 28, 28)\n",
    "    data.append(image)\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(y=labels, x = [labels.count(i) for i in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the raw pixels to range [0,1] since NN require this range\n",
    "data = np.array(data, dtype=\"float\")/255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate data to train and test\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the labels to one hot encodings\n",
    "trainY = LabelBinarizer.fit_transform(trainY)\n",
    "testY = LabelBinarizer.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inititalize the model\n",
    "model = minivggnet.build(width=28, height=28, depth=1, classes=9)\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=32, epochs= 15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"...\\best_capt.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0,15), H.hisstory[\"loss\"], labels=\"train_loss\")\n",
    "plt.plot(np.arange(0,15), H.history[\"val_loss\"], label = \"val_loss\")\n",
    "plt.plot(np.arange(0,15), H.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(np.arange(0, 15), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244139a3b14db007857c14f2a9d5e3b23ed735eeb2f38a787fb45edaaccb434b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
