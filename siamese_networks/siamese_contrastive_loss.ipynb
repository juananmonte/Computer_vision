{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As always, import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----Utils-----\n",
    "from pyimagesearch.siamese_network import build_siamese_model\n",
    "from pyimagesearch import utils\n",
    "#-----Tensorflow-----\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda, Dense, Input\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "#-----Others-------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the shape of the inputs for our network\n",
    "IMG_SHAPE = (28, 28, 1)\n",
    "\n",
    "# specify the batch size and number of epochs\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"siamese_model\"])\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "#Scaling the images\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "\n",
    "#Add extra channel. This is due to the make_pairs function (check utils.py for more details)\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pairTrain, labelTrain) = utils.make_pairs(trainX, trainY)\n",
    "(pairTest, labelTest) = utils.make_pairs(testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Siamese network and the inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 12:25:16.280511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:16.338533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:16.339122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:16.341418: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-16 12:25:16.343143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:16.344537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:16.345068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:17.839502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:17.840196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:17.840725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-16 12:25:17.841040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5336 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "#Define the inputs (image A and B) of the Siamese network\n",
    "imgA = Input(shape=IMG_SHAPE)\n",
    "imgB = Input(shape=IMG_SHAPE)\n",
    "\n",
    "#We initialize the network but we take it as a feature extractor. This is to obey one of the principles pf Siamese Networks: same weights updates\n",
    "#If we give first imgA as input and then imgB then the weights will be different\n",
    "feature_extractor = build_siamese_model(IMG_SHAPE)\n",
    "featsA = feature_extractor(imgA)\n",
    "featsB = feature_extractor(imgB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we have the features, we pass those through the euclidian distance (to calculate similarity) and then into the Dense Layer to get the output\n",
    "#This is using the functional API\n",
    "distance = Lambda(utils.euclidean_distance)([featsA, featsB])\n",
    "#outputs = Dense(1, activation= \"sigmoid\")(distance) We have to use the distance itself as the output.If not, the loss will be the same of both train and test\n",
    "model = Model(inputs=[imgA, imgB], outputs=distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y, preds, margin=1):\n",
    "\t# explicitly cast the true class label data type to the predicted class label data type\n",
    "\ty = tf.cast(y, preds.dtype)\n",
    "\n",
    "\t# calculate the contrastive loss between the true labels and the predicted labels\n",
    "\tsquaredPreds = K.square(preds)\n",
    "\tsquaredMargin = K.square(K.maximum(margin - preds, 0))\n",
    "\tloss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
    "\n",
    "\t# return the computed contrastive loss to the calling function\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=contrastive_loss, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2665 - val_loss: 0.2675\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2354 - val_loss: 0.2589\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2320 - val_loss: 0.2696\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2310 - val_loss: 0.2624\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2284 - val_loss: 0.2512\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2267 - val_loss: 0.2575\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2255 - val_loss: 0.2547\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.2249 - val_loss: 0.2458\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2242 - val_loss: 0.2545\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2229 - val_loss: 0.2519\n"
     ]
    }
   ],
   "source": [
    "#Give the positive images (1) and negatives(0) into a nested list\n",
    "history = model.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:], validation_data=([pairTest[:, 0], pairTest[:, 1]], labelTest[:]),\n",
    "\tbatch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/siamese_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/siamese_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da695c087d6368c9b262a08c2ebf368542a47f83b2881693eefdf5ebab9d0ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
