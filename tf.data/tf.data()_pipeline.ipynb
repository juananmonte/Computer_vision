{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---tensorflow\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.data import AUTOTUNE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "#---sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "#---others\n",
    "from cancer_model.cancernet import CancerNet\n",
    "from imutils import paths\n",
    "import imutils \n",
    "import random\n",
    "import cv2\n",
    "import shutil\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check some pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"....tf_data\\\\cancer_data\\\\\"\n",
    "all_images = list(paths.list_images(main_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = random.choices(all_images, k=3)\n",
    "for i in random_images:\n",
    "    random_image = cv2.imread(i)\n",
    "    random_image = cv2.cvtColor(random_image, cv2.COLOR_BGR2RGB)\n",
    "    random_image = imutils.resize(random_image, height=200)\n",
    "    cv2.imshow(\"example\", random_image)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide the data in train, test and validation (easier than train_test_split and goes easy on disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test\n",
    "i = int(len(all_images)*0.8) #80% of data to training set\n",
    "trainData = all_images[:i]\n",
    "testData = all_images[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "i = int(len(trainData)*0.1)\n",
    "validData = trainData[:i]\n",
    "trainData = trainData[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```another option is to use the splitfolders library which divides the data in training, validation and test on folders on disk. But this only applies when we have all data together in one folder```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract labels and images and save them on folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Folders\n",
    "train_path = main_path+\"trainig\"\n",
    "valid_path = main_path+\"validation\"\n",
    "test_path = main_path+\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(\"training\", trainData, train_path ), (\"validation\", validData, valid_path), \n",
    "            (\"testing\", testData, test_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dtype, imagepaths, out_path) in datasets:\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    for inputpath in imagepaths:\n",
    "        filename = inputpath.split(os.path.sep)[-1]\n",
    "        label = inputpath.split(os.path.sep)[-2] #os.path.sep refers to the \"\\\\\"symbols.So split by this.\n",
    "    \n",
    "        #Now that we have the labels, create a path with those labels for each dataset\n",
    "        labelPath = os.path.sep.join([out_path, label])\n",
    "\n",
    "        if not os.path.exists(labelPath):\n",
    "            os.makedirs(labelPath)\n",
    "\n",
    "        #get the image and copy it to the respecive label folder\n",
    "        p = os.path.sep.join([labelPath, filename])\n",
    "        shutil.copy2(inputpath, p)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy tf.data() pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------First! create the preprocess pipeline\n",
    "def load_images(imagePath):\n",
    "    #--images\n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.io.decode_png(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype= tf.float32)\n",
    "    image = tf.image.resize(image, (48,48))\n",
    "    #--labels\n",
    "    label = tf.strings.split(imagePath, os.path.sep)[-2]\n",
    "    label = tf.strings.to_number(label, tf.int32)\n",
    "    return(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function #the decorator makes this function able to be used with tf on the pipeline\n",
    "#-------Second! the augmentation pipeline\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quickly get the images from each path\n",
    "trainPaths = list(paths.list_images(train_path))\n",
    "valPaths = list(paths.list_images(valid_path))\n",
    "testPaths = list(paths.list_images(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the tf.data() pipeline for each dataset\n",
    "\n",
    "#--Train\n",
    "trainDS = tf.data.Dataset.from_tensor_slices(trainPaths)\n",
    "trainDS = (trainDS.shuffle(len(trainPaths)).map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "          .map(augment, num_parallel_calls=AUTOTUNE).cache().batch(64).prefetch(AUTOTUNE)\n",
    "          )\n",
    "\n",
    "#--Validation\n",
    "valDS = tf.data.Dataset.from_tensor_slices(valPaths)\n",
    "valDS = (valDS.map(load_images, num_parallel_calls=AUTOTUNE).cache().batch(64).prefetch(AUTOTUNE))\n",
    "#Remember not to shuffle and augment the validation and test, but as always we have to preprocess it\n",
    "\n",
    "#--Test\n",
    "testDS = tf.data.Dataset.from_tensor_slices(testPaths)\n",
    "testDS = (testDS.map(load_images, num_parallel_calls=AUTOTUNE).cache().batch(64).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with the class imabalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the labels from the training set\n",
    "train_paths = list(paths.list_images(train_path))\n",
    "trainlabels = [int(p.split(os.path.sep)[-2]) for p in train_paths]\n",
    "#turn them into categorical values so we can count them (one hot encoding)\n",
    "trainlabels = to_categorical(trainlabels)\n",
    "classTotals = trainlabels.sum(axis=0)\n",
    "classWeight = {}\n",
    "#loop though all the sum and calculate the weights\n",
    "for i in range(0, len(classTotals)):\n",
    "    classWeight[i] = classTotals.max()/classTotals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 2.5295885}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our CancerNet model and compile it\n",
    "model = CancerNet.build(width=48, height=48, depth=3, classes=1)\n",
    "opt = Adagrad(learning_rate=1e-2, decay=1e-2/40)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before fit, define some callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_s = EarlyStopping(monitor=\"val_loss\", patience = 10, restore_best_weights=True)\n",
    "save_b = ModelCheckpoint(filepath =\"...tf_data\\\\cancer_model\",\n",
    "                         monitor=\"val_loss\", verbose = 1 )\n",
    "callbacks = [early_s, save_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "313/313 [==============================] - 17s 45ms/step - loss: 0.7542 - accuracy: 0.7666 - val_loss: 3.8432 - val_accuracy: 0.2932\n",
      "\n",
      "Epoch 00001: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 2/60\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.6458 - accuracy: 0.8065 - val_loss: 1.2227 - val_accuracy: 0.4874\n",
      "\n",
      "Epoch 00002: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 3/60\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6201 - accuracy: 0.8146 - val_loss: 0.6001 - val_accuracy: 0.7320\n",
      "\n",
      "Epoch 00003: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 4/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5995 - accuracy: 0.8221 - val_loss: 0.4373 - val_accuracy: 0.8302\n",
      "\n",
      "Epoch 00004: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 5/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5833 - accuracy: 0.8274 - val_loss: 0.4610 - val_accuracy: 0.7941\n",
      "\n",
      "Epoch 00005: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 6/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5709 - accuracy: 0.8320 - val_loss: 0.5081 - val_accuracy: 0.7941\n",
      "\n",
      "Epoch 00006: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 7/60\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5644 - accuracy: 0.8345 - val_loss: 0.4940 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00007: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 8/60\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.5608 - accuracy: 0.8354 - val_loss: 0.4553 - val_accuracy: 0.8216\n",
      "\n",
      "Epoch 00008: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 9/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5553 - accuracy: 0.8339 - val_loss: 0.4889 - val_accuracy: 0.7946\n",
      "\n",
      "Epoch 00009: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 10/60\n",
      "313/313 [==============================] - 12s 36ms/step - loss: 0.5505 - accuracy: 0.8387 - val_loss: 0.4230 - val_accuracy: 0.8338\n",
      "\n",
      "Epoch 00010: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 11/60\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.5480 - accuracy: 0.8362 - val_loss: 0.4832 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00011: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 12/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5415 - accuracy: 0.8396 - val_loss: 0.4680 - val_accuracy: 0.8126\n",
      "\n",
      "Epoch 00012: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 13/60\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.5385 - accuracy: 0.8392 - val_loss: 0.4763 - val_accuracy: 0.8059\n",
      "\n",
      "Epoch 00013: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 14/60\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.5376 - accuracy: 0.8405 - val_loss: 0.4768 - val_accuracy: 0.8086\n",
      "\n",
      "Epoch 00014: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 15/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5321 - accuracy: 0.8418 - val_loss: 0.4262 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00015: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 16/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5319 - accuracy: 0.8430 - val_loss: 0.4717 - val_accuracy: 0.8018\n",
      "\n",
      "Epoch 00016: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 17/60\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.5305 - accuracy: 0.8419 - val_loss: 0.4907 - val_accuracy: 0.7950\n",
      "\n",
      "Epoch 00017: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 18/60\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.5288 - accuracy: 0.8438 - val_loss: 0.4529 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00018: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 19/60\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.5224 - accuracy: 0.8460 - val_loss: 0.4657 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00019: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n",
      "Epoch 20/60\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.5217 - accuracy: 0.8474 - val_loss: 0.4622 - val_accuracy: 0.8108\n",
      "\n",
      "Epoch 00020: saving model to G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\n",
      "INFO:tensorflow:Assets written to: G:\\pyimage_univ\\CNN_tf\\tf_data\\cancer_model\\assets\n"
     ]
    }
   ],
   "source": [
    "#No need for batch size since is defined on the tf.data() pipeline\n",
    "H = model.fit(x = trainDS, validation_data=valDS, class_weight=classWeight, epochs= 60, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"training loss and accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"loss/accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, get labels of the test set\n",
    "test_paths = list(paths.list_images(test_path))\n",
    "testlabels = [int(p.split(os.path.sep)[-2]) for p in test_paths]\n",
    "#turn them into categorical values so we can count them (one hot encoding)\n",
    "testlabels = to_categorical(testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86     41781\n",
      "           1       0.00      0.00      0.00     13724\n",
      "\n",
      "    accuracy                           0.75     55505\n",
      "   macro avg       0.38      0.50      0.43     55505\n",
      "weighted avg       0.57      0.75      0.65     55505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testDS)\n",
    "\n",
    "print(classification_report(testlabels.argmax(axis=1), predictions.argmax(axis=1), target_names=[\"0\", \"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"...tf_data\\\\cancer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(testDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NewRandomAccessFile failed to Create/Open: ...cancer_data\\test\\1\\9041_idx5_x2851_y1951_class1.png : The system cannot find the path specified.\r\n; No such process [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10412/4028636300.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# (and since we used the tf.data() pipeline we have to pass the image through the same process):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"...cancer_data\\\\test\\\\1\\\\9041_idx5_x2851_y1951_class1.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;34m\"string\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \"\"\"\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    552\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m       return read_file_eager_fallback(\n\u001b[0m\u001b[0;32m    555\u001b[0m           filename, name=name, ctx=_ctx)\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[1;34m(filename, name, ctx)\u001b[0m\n\u001b[0;32m    575\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m    578\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m    579\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\juana\\.conda\\envs\\gpu-dl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: NewRandomAccessFile failed to Create/Open: ...cancer_data\\test\\1\\9041_idx5_x2851_y1951_class1.png : The system cannot find the path specified.\r\n; No such process [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "#All the predict functions in Keras expect batch of inputs. Therefore, since you are doing prediction on one single image, \n",
    "# you need to add an axis at the beginning of image tensor to represent the batch axis \n",
    "# (and since we used the tf.data() pipeline we have to pass the image through the same process):\n",
    "image = \"...cancer_data\\\\test\\\\1\\\\9041_idx5_x2851_y1951_class1.png\"\n",
    "image = tf.io.read_file(image)\n",
    "image = tf.image.decode_jpeg(image, channels=3)\n",
    "image = tf.image.resize(image, [48, 48])\n",
    "image = tf.expand_dims(image, axis=0)   # the shape would be (1, 48, 48, 3)  \n",
    "print(np.argmax(model.predict(image), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244139a3b14db007857c14f2a9d5e3b23ed735eeb2f38a787fb45edaaccb434b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
