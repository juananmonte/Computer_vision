{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Pytorch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch import nn\n",
    "import torch\n",
    "#---Others\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the configuration parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    DATA_PATH = \"flower_photos\" #where the pictures are\n",
    "    BASE_PATH = \"dataset\" #the new path of the dataset. Ex: dataset/flower_class/picture1.jpg\n",
    "\n",
    "    VAL_SPLIT = 0.1\n",
    "    TEST_SPLIT = 0.1\n",
    "    TRAIN = os.path.join(BASE_PATH, \"train\")\n",
    "    VAL = os.path.join(BASE_PATH, \"val\")\n",
    "    TEST = os.path.join(BASE_PATH, \"test\")\n",
    "\n",
    "    #This mean and standard deviation values were used to train the resnet50 model. \n",
    "    # they are defined for the each of the RGB channels\n",
    "    # We need them to be the same so we don't encounter problems when normalizing the data and then feeding it to the model\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    FEATURE_EXTRACTION_BATCH_SIZE = 256\n",
    "    FINETUNE_BATCH_SIZE = 64\n",
    "    PRED_BATCH_SIZE = 4\n",
    "    EPOCHS = 20\n",
    "    LR = 0.001\n",
    "    LR_FINETUNE = 0.0005\n",
    "\n",
    "    WARMUP_PLOT = os.path.join(\"output\", \"warmup.png\")\n",
    "    FINETUNE_PLOT = os.path.join(\"output\", \"finetune.png\")\n",
    "    WARMUP_MODEL = os.path.join(\"output\", \"warmup_model.pth\")\n",
    "    FINETUNE_MODEL = os.path.join(\"output\", \"finetune_model.pth\")\n",
    "\n",
    "#Instantiate the class (so we can access al the parameters as methods)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and foremost: get the data and organize it in train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(Config.DATA_PATH))\n",
    "np.random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roses'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check \n",
    "str.split(imagePaths[0], '/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(imagePath, folder):\n",
    "    #for a folder (train, val or test) that doesn't exist, create it\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    for path in imagePath:\n",
    "        imagename = os.path.split(os.path.sep)[-1]\n",
    "        label = str.split(path, \"/\")[1]\n",
    "        new_path = os.path.join(folder, label)\n",
    "        print(new_path)\n",
    "\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "        \n",
    "        #Copy each image to the new destination\n",
    "        destination = os.path.join(new_path, imagename)\n",
    "        shutil.copy(path, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the proportions of how much data goes to each folder\n",
    "testPathlen = int(len(imagePaths) * config.TEST_SPLIT)\n",
    "difference_test = int(len(imagePaths) - testPathlen)\n",
    "trainPaths = imagePaths[:difference_test]\n",
    "testPaths = imagePaths[difference_test:]\n",
    "\n",
    "#Now validation\n",
    "valPathlen = int(len(trainPaths)*config.VAL_SPLIT)\n",
    "difference_val = int(len(trainPaths) - valPathlen)\n",
    "trainpaths = trainPaths[:difference_val]\n",
    "valPaths = trainPaths[difference_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_images(trainPaths, config.TRAIN)\n",
    "# copy_images(trainPaths, config.VAL)\n",
    "# copy_images(trainPaths, config.TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data, define DataLoader and pass transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(rootDir, transforms, batchSize, shuffle=True):\n",
    "    ds = datasets.ImageFolder(rootDir, transform=transforms) #get the data and make transformations(ex: normalize, resize, augmentations)\n",
    "    loader = DataLoader(ds, batch_size=batchSize, shuffle=shuffle, num_workers=os.cpu_count(), pin_memory= True if config.DEVICE == \"cuda\" else False) #move the dataset to dataloader\n",
    "    return(ds, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the tranformations to do\n",
    "\n",
    "#For training data\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(), #necessary for all3 datasets\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD)]) #necessary for all 3 datasets\n",
    "\n",
    "#For validation\n",
    "valTransform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD),\n",
    "    ])\n",
    "\n",
    "#For test\n",
    "testTransform =  transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std = config.STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function for data loader defined previously and pass the datasets\n",
    "\n",
    "#For training\n",
    "(trainDS, trainLoader) = get_data_loader(config.TRAIN, transforms=trainTransform, batchSize=config.FINETUNE_BATCH_SIZE, shuffle=True) \n",
    "#For validation\n",
    "(valDS, valLoader) = get_data_loader(config.VAL, transforms= valTransform, batchSize= config.FINETUNE_BATCH_SIZE, shuffle=False) #remember to not shuffle\n",
    "#For test\n",
    "(testDS, testLoader) = get_data_loader(config.TEST, transforms= testTransform, batchSize= config.FINETUNE_BATCH_SIZE, shuffle= False) #same here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning via feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/gpu-dl/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model = resnet50(weights=True)\n",
    "\n",
    "#Due to this being feature extraction, we set the parameters as non trainable. This is done in order to use the already trained filters/parameters to analyze the new data\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we append a new fully connected layer(s) on top of the model\n",
    "modelOutputFeats = model.fc.in_features #get the output\n",
    "model.fc = nn.Linear(modelOutputFeats, len(trainDS.classes))\n",
    "model = model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the loss function and optimizer\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.fc.parameters(), lr=config.LR) #use only the parameters of the top part of the NN. the one we are defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate steps per epoch \n",
    "trainSteps = len(trainDS)//config.FEATURE_EXTRACTION_BATCH_SIZE\n",
    "valSteps = len(valDS)//config.FEATURE_EXTRACTION_BATCH_SIZE\n",
    "testSteps = len(testDS)//config.FEATURE_EXTRACTION_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [02:42<51:32, 162.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/20\n",
      "Train loss: 4.762776, Train accuracy: 0.6076\n",
      "Val loss: 2.927257, Val accuracy: 0.7829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [03:14<25:43, 85.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/20\n",
      "Train loss: 3.042628, Train accuracy: 0.7620\n",
      "Val loss: 2.340154, Val accuracy: 0.8093\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "for e in tqdm(range(config.EPOCHS)):\n",
    "    model.train()\n",
    "\n",
    "    totalTrainloss = 0\n",
    "    totalValloss = 0\n",
    "\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    for (i, (x, y)) in enumerate(trainLoader):\n",
    "        (x,y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = lossFunc(pred, y)\n",
    "\n",
    "        #Code the 3 important steps\n",
    "        loss.backward()\n",
    "\n",
    "        # check if we are updating the model parameters and if so\n",
    "\t\t# update them\n",
    "        if (i+2)%2==0:\n",
    "            opt.step()\n",
    "            opt.zero_grad() \n",
    "         \n",
    "        totalTrainloss += loss\n",
    "        trainCorrect += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for (x,y) in valLoader:\n",
    "            (x,y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "            \n",
    "            pred = model(x)\n",
    "            totalValloss += lossFunc(pred, y)\n",
    "\n",
    "            valCorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainloss / trainSteps\n",
    "    avgValLoss = totalValloss / valSteps\n",
    "\n",
    "\t# calculate the training and validation accuracy\n",
    "    trainCorrect = trainCorrect / len(trainDS)\n",
    "    valCorrect = valCorrect / len(valDS)\n",
    "\n",
    "\n",
    "\t# print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, config.EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\tavgTrainLoss, trainCorrect))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\".format(avgValLoss, valCorrect))\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
    "\n",
    "torch.save(model, config.FINETUNE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da695c087d6368c9b262a08c2ebf368542a47f83b2881693eefdf5ebab9d0ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
