{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Pytorch \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch import nn\n",
    "import torch\n",
    "#--other\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is already built. You can check the \"Transfer_learning_via_feature_extraction\" file to get more details on how to build it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    DATA_PATH = \"flower_photos\" #where the pictures are\n",
    "    BASE_PATH = \"dataset\" #the new path of the dataset. Ex: dataset/flower_class/picture1.jpg\n",
    "\n",
    "    VAL_SPLIT = 0.1\n",
    "    TEST_SPLIT = 0.1\n",
    "    TRAIN = os.path.join(BASE_PATH, \"train\")\n",
    "    VAL = os.path.join(BASE_PATH, \"val\")\n",
    "    TEST = os.path.join(BASE_PATH, \"test\")\n",
    "\n",
    "    #This mean and standard deviation values were used to train the resnet50 model. \n",
    "    # they are defined for the each of the RGB channels\n",
    "    # We need them to be the same so we don't encounter problems when normalizing the data and then feeding it to the model\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    FEATURE_EXTRACTION_BATCH_SIZE = 256\n",
    "    FINETUNE_BATCH_SIZE = 64\n",
    "    PRED_BATCH_SIZE = 4\n",
    "    EPOCHS = 20\n",
    "    LR = 0.001\n",
    "    LR_FINETUNE = 0.0005\n",
    "\n",
    "    WARMUP_PLOT = os.path.join(\"output\", \"warmup.png\")\n",
    "    FINETUNE_PLOT = os.path.join(\"output\", \"finetune.png\")\n",
    "    WARMUP_MODEL = os.path.join(\"output\", \"warmup_model.pth\")\n",
    "    FINETUNE_MODEL = os.path.join(\"output\", \"finetune_model.pth\")\n",
    "\n",
    "#Instantiate the class (so we can access all the parameters as methods)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly check some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(config.TRAIN))\n",
    "random_i = random.choices(imagePaths, k=5)\n",
    "\n",
    "for path in random_i:\n",
    "    im = cv2.imread(path)\n",
    "    #im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR) #due to linux\n",
    "    label = str.split(path, \"/\")[2]\n",
    "    cv2.imshow(label, im)\n",
    "    cv2.waitKey()\n",
    "    \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for loading data, DataLaoder and pass transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(rootDir, transforms, batch_size, shuffle=True):\n",
    "    ds = datasets.ImageFolder(rootDir, transform=transforms)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=os.cpu_count(), pin_memory=True if config.DEVICE == \"cuda\" else False)\n",
    "    return(ds, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training data\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])\n",
    "#For validation\n",
    "valTransform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])\n",
    "#For test\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "(trainDS, trainLoader) = get_data_loader(config.TRAIN, transforms=trainTransform, batch_size=config.FINETUNE_BATCH_SIZE)\n",
    "#Val\n",
    "(valDS, valLoader) = get_data_loader(config.VAL, transforms=valTransform, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=False)\n",
    "#test\n",
    "(testDS, testLoader) = get_data_loader(config.TEST, transforms=testTransform, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/gpu-dl/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#get themodel\n",
    "\n",
    "model = resnet50(weights=True)\n",
    "numfeatures = model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the modules of the model and set the parameters of  batch normalization modules as not trainable\n",
    "for module, param in zip(model.modules(), model.parameters()):\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the new head to attach to the model\n",
    "headModel = nn.Sequential(\n",
    "    nn.Linear(numfeatures, 512), #here we connect the previous paramaters\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, len(trainDS.classes))\n",
    ")\n",
    "model.fc = headModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config.LR)\n",
    "\n",
    "#Steps per epochs\n",
    "trainSteps = len(trainDS)//config.FINETUNE_BATCH_SIZE\n",
    "valSteps = len(valDS)//config.FINETUNE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [],\n",
    "\t\"val_acc\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 5.94 GiB total capacity; 5.24 GiB already allocated; 3.75 MiB free; 5.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000023untitled?line=9'>10</a>\u001b[0m valCorrect \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000023untitled?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m (i, (x,y)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainLoader):\n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000023untitled?line=12'>13</a>\u001b[0m     (x,y) \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mDEVICE), y\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mDEVICE))\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000023untitled?line=14'>15</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000023untitled?line=15'>16</a>\u001b[0m     loss \u001b[39m=\u001b[39m lossFunc(pred, y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 5.94 GiB total capacity; 5.24 GiB already allocated; 3.75 MiB free; 5.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "for e in tqdm(range(20)):\n",
    "    model.train()\n",
    "\n",
    "    totalTrainLoss = 0\n",
    "    totalValloss = 0\n",
    "\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    for (i, (x,y)) in enumerate(trainLoader):\n",
    "        (x,y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = lossFunc(pred, y)\n",
    "\n",
    "    #Define 3 important steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i+2) %2 ==0:\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        totalTrainLoss += loss\n",
    "        trainCorrect += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for (x, y) in valLoader:\n",
    "            (x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "\n",
    "            pred = model(x)\n",
    "            totalValloss += lossFunc(pred, y)\n",
    "\n",
    "            valCorrect += (pred.argmax(1) == y).type(torch.flat).sum().item()\n",
    "\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValloss / valSteps\n",
    "\n",
    "    trainCorrect = trainCorrect/len(trainDS)\n",
    "    valCorrect =valCorrect/len(valDS)\n",
    "\n",
    "\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy()) #only for losses\n",
    "    H[\"train_acc\"].append(trainCorrect)\n",
    "    H[\"train_loss\"].append(avgValLoss.cpu().detach().numpy()) #only for losses\n",
    "    H[\"train_acc\"].append(valCorrect)\n",
    "\n",
    "    print(\"Epoch: {}/{}\".format(e+1, config.EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
    "    print(\"Val loss {:.6f}, Val accuracy: {:.4f}\".format(avgValLoss, valCorrect))\n",
    "\n",
    "endTime = time.time()\n",
    "\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(config.FINETUNE_PLOT)\n",
    "\n",
    "# serialize the model to disk\n",
    "torch.save(model, config.FINETUNE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gpu-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da695c087d6368c9b262a08c2ebf368542a47f83b2881693eefdf5ebab9d0ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
